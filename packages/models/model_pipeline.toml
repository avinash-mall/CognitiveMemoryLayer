# ============================================================================
# Unified Model Pipeline Configuration
# ============================================================================
# This file controls BOTH:
#   1) data preparation for all model families
#   2) training/evaluation for all model families
#
# Design goal:
# - one prepare script (`packages/models/scripts/prepare.py`)
# - one train script (`packages/models/scripts/train.py`)
#
# All paths can be absolute OR relative to repository root.
# ============================================================================

[paths]
# Directory for prepared parquet outputs created by prepare.py
prepared_dir = "packages/models/prepared_data/modelpack"

# Directory containing legacy/bootstrapped prepared splits used as weak labels
# Expected files (optional): train.parquet, test.parquet, eval.parquet
bootstrap_prepared_dir = "packages/models/prepared_data"

# Final model output directory used by train.py
# User requirement: all models must be written here.
trained_models_dir = "packages/models/trained_models"

# Dataset storage/caching directory used by `datasets` package.
# Uses existing project folder (flat raw datasets can coexist here).
datasets_cache_dir = "packages/models/datasets"

[prepare]
# Global random seed for sampling/shuffling/splits
seed = 42

# Split ratios for stratified split by (task,label)
train_ratio = 0.80
test_ratio = 0.10
eval_ratio = 0.10

# Single knob for dataset size: target number of samples per (task, label).
# target_per_task_label, max_per_task_label, and max_rows_per_source are derived
# from this in prepare.py when not set explicitly below.
samples_per_task_label = 5000

# Optional overrides (derived from samples_per_task_label when unset):
# max_rows_per_source = 6000
# target_per_task_label = 100
# max_per_task_label = 5000

# If true, required remote datasets are auto-downloaded when missing
# (through Hugging Face datasets loader).
auto_download_missing = true

# If true, missing `datasets` package is treated as an error.
# Set false only for strictly local-only prep runs.
require_datasets_package = true

[train]
# Random seed for model training
seed = 42

# Families to train. Keep all 3 for complete replacement.
families = ["router", "extractor", "pair"]

# TF-IDF settings
max_features = 250000
min_df = 2
ngram_min = 1
ngram_max = 2

# SGDClassifier settings
# `max_iter` behaves like epochs for SGD over sparse features.
max_iter = 25
alpha = 0.00001

# Prediction batch size for progress/evaluation
predict_batch_size = 8192

[synthetic_llm]
# Synthetic generation is LLM-only.
# Provider/model/base URL are loaded from env first.
#
# Required env values (user preference):
#   LLM_EVAL__PROVIDER=ollama
#   LLM_EVAL__MODEL=gemma3:12b
#   LLM_EVAL__BASE_URL=http://localhost:11434/v1
provider_env = "LLM_EVAL__PROVIDER"
model_env = "LLM_EVAL__MODEL"
base_url_env = "LLM_EVAL__BASE_URL"
provider = "ollama"
model = ""
base_url = ""
api_key_env = "OPENAI_API_KEY"

# Higher randomness for diversity.
temperature = 1.35
top_p = 0.95
max_tokens = 1024
batch_size = 24
timeout_seconds = 120
max_retries = 3

# Retry budget per (task,label) while filling missing rows.
max_attempts_per_label = 80

# Only the first N rows/lines from each local raw dataset file are scanned
# to build seed pools (fast structure probing).
local_raw_scan_rows_per_file = 300

# ============================================================================
# Remote datasets (auto-downloaded by prepare.py when enabled=true)
# ============================================================================
# Fields:
# - name: unique key used by preparation logic
# - target: which model family primarily consumes this dataset
# - kind: currently supports "huggingface"
# - link: human-readable reference URL (docs / source provenance)
# - dataset_id: HF dataset identifier
# - config: optional HF config/subset
# - split: HF split name (train/validation/test)
# - max_rows: optional per-dataset cap override
# - enabled: include this dataset in pipeline
# - required: fail prep if download/load fails

[[datasets]]
name = "banking77"
target = "router"
kind = "huggingface"
link = "https://huggingface.co/datasets/banking77"
dataset_id = "banking77"
config = ""
split = "train"
max_rows = 60000
enabled = true
required = true

[[datasets]]
name = "ag_news"
target = "router"
kind = "huggingface"
link = "https://huggingface.co/datasets/ag_news"
dataset_id = "ag_news"
config = ""
split = "train"
max_rows = 12000
enabled = true
required = true

[[datasets]]
name = "clinc_oos"
target = "router"
kind = "huggingface"
link = "https://huggingface.co/datasets/clinc_oos"
dataset_id = "clinc_oos"
config = "plus"
split = "train"
max_rows = 50000
enabled = true
required = true

[[datasets]]
name = "dbpedia_14"
target = "router"
kind = "huggingface"
link = "https://huggingface.co/datasets/dbpedia_14"
dataset_id = "dbpedia_14"
config = ""
split = "train"
max_rows = 60000
enabled = true
required = true

[[datasets]]
name = "moral_stories"
target = "extractor"
kind = "huggingface"
link = "https://huggingface.co/datasets/go_emotions"
dataset_id = "go_emotions"
config = "simplified"
split = "train"
max_rows = 50000
enabled = true
required = true

[[datasets]]
name = "pii_masking"
target = "extractor"
kind = "huggingface"
link = "https://huggingface.co/datasets/ai4privacy/pii-masking-200k"
dataset_id = "ai4privacy/pii-masking-200k"
config = ""
split = "train"
max_rows = 120000
enabled = true
required = true

[[datasets]]
name = "snli"
target = "pair"
kind = "huggingface"
link = "https://huggingface.co/datasets/stanfordnlp/snli"
dataset_id = "stanfordnlp/snli"
config = ""
split = "train"
max_rows = 200000
enabled = true
required = true

[[datasets]]
name = "multi_nli"
target = "pair"
kind = "huggingface"
link = "https://huggingface.co/datasets/nyu-mll/multi_nli"
dataset_id = "nyu-mll/multi_nli"
config = ""
split = "train"
max_rows = 200000
enabled = true
required = true

[[datasets]]
name = "anli"
target = "pair"
kind = "huggingface"
link = "https://huggingface.co/datasets/facebook/anli"
dataset_id = "facebook/anli"
config = ""
split = "train_r3"
max_rows = 100000
enabled = true
required = true

[[datasets]]
name = "ms_marco"
target = "pair"
kind = "huggingface"
link = "https://huggingface.co/datasets/microsoft/ms_marco"
dataset_id = "microsoft/ms_marco"
config = "v1.1"
split = "train"
max_rows = 120000
enabled = true
required = true

[[datasets]]
name = "ultrachat_200k"
target = "seed"
kind = "huggingface"
link = "https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k"
dataset_id = "HuggingFaceH4/ultrachat_200k"
config = ""
split = "train_sft"
max_rows = 80000
enabled = true
required = false

[[datasets]]
name = "openassistant_oasst1"
target = "seed"
kind = "huggingface"
link = "https://huggingface.co/datasets/OpenAssistant/oasst1"
dataset_id = "OpenAssistant/oasst1"
config = ""
split = "train"
max_rows = 80000
enabled = true
required = false

[[datasets]]
name = "quora_duplicates"
target = "pair"
kind = "huggingface"
link = "https://huggingface.co/datasets/sentence-transformers/quora-duplicates"
dataset_id = "sentence-transformers/quora-duplicates"
config = "pair-class"
split = "train"
max_rows = 120000
enabled = true
required = true
