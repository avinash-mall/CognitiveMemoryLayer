# Evaluation script and CML API config (optional).
# Copy to .env in project root for the API; set these when running the eval script if not using defaults.
# For full LoCoMo ingestion, in project root .env set AUTH__RATE_LIMIT_REQUESTS_PER_MINUTE to 600 or higher.
# Embedding model and dimensions come from EMBEDDING__* in project root .env.

# CML API (must match AUTH__API_KEY used by the server)
CML_BASE_URL=http://localhost:8000
CML_API_KEY=test-key

# QA step (Phase B) uses the same LLM as the rest of the project — see project root .env.example lines 45–49.
# Set in project root .env (not here). Example for OpenAI:
#   LLM__PROVIDER=openai
#   LLM__MODEL=gpt-4o-mini
#   LLM__API_KEY=sk-...
# Example for Ollama (OpenAI-compatible endpoint):
#   LLM__PROVIDER=ollama
#   LLM__MODEL=gpt-oss:20b
#   LLM__BASE_URL=http://localhost:11434/v1
