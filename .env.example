# =============================================================================
# Cognitive Memory Layer (CML) â€” Environment Configuration
# =============================================================================
# Copy to .env and set values. [Required] = must set. [Optional] = has default.
# Migration: If you used MEMORY_API_URL, MEMORY_API_KEY, or MEMORY_API_TIMEOUT, set CML_BASE_URL, CML_API_KEY, and CML_TIMEOUT instead (no fallbacks).
# =============================================================================

# ---------- PACKAGE VERSION (build) ----------
# VERSION â€” [Optional] Package version for Hatch build. Used by: pyproject.toml (dynamic version). Fallback: VERSION env or repo VERSION file. Bump when releasing.
# VERSION=1.3.6

# =============================================================================
# PART 1 â€” CML SERVER (API) â€” from src/core/config.py
# =============================================================================
# Server reads settings via pydantic-settings. Nested keys use double
# underscore: SECTION__KEY (e.g. DATABASE__POSTGRES_URL). Values match
# docker-compose for local development where relevant.
# =============================================================================

# ---------- Database ----------
# Docker: set POSTGRES_*, NEO4J_USER, NEO4J_PASSWORD; compose injects DATABASE__* for the app. Local: set only DATABASE__* below (host localhost).
# POSTGRES_USER=memory
# POSTGRES_PASSWORD=memory
# POSTGRES_DB=memory
# NEO4J_USER=neo4j
# NEO4J_PASSWORD=password

# App (config.py â†’ DatabaseSettings). Required.
DATABASE__POSTGRES_URL=postgresql+asyncpg://memory:memory@localhost/memory

# ---------- REQUIRED ----------

# AUTH__API_KEY â€” [Required] API key for memory operations (X-API-Key header). Used by: config.py â†’ AuthSettings; src/api/auth.py. Use test-key for local dev; set strong secret in production. To run the API with test keys without editing .env, use docker compose -f docker/docker-compose.yml -f docker/docker-compose.test-key.yml up api.
AUTH__API_KEY=test-key

# ---------- OPTIONAL BUT HIGHLY RECOMMENDED ----------

# DATABASE__NEO4J_URL â€” [Optional] Neo4j Bolt URL. Used by: config.py; src/storage/neo4j.py. Local: bolt://localhost:7687. Docker: compose overrides bolt://neo4j:7687.
# DATABASE__NEO4J_URL=bolt://localhost:7687

# DATABASE__NEO4J_USER â€” [Optional] Neo4j username. Docker: compose overrides from NEO4J_USER.
# DATABASE__NEO4J_USER=neo4j

# DATABASE__NEO4J_PASSWORD â€” [Optional] Neo4j password. Docker: compose overrides from NEO4J_PASSWORD.
# DATABASE__NEO4J_PASSWORD=password

# DATABASE__NEO4J_BROWSER_URL â€” [Optional] Neo4j Bolt URL for dashboard (neovis.js). When API in Docker, browser on host: bolt://localhost:7687. Else same as DATABASE__NEO4J_URL.
# DATABASE__NEO4J_BROWSER_URL=bolt://localhost:7687

# DATABASE__REDIS_URL â€” [Optional] Redis URL. Used by: config.py; src/storage/redis.py. Local: redis://localhost:6379. Docker: compose overrides redis://redis:6379.
# DATABASE__REDIS_URL=redis://localhost:6379

# ---------- OPTIONAL (defaults shown) ----------

# AUTH__ADMIN_API_KEY â€” [Optional] Admin API key for dashboard, consolidate, run_forgetting, list_tenants. Default: None. Used by: config.py; auth.py.
# AUTH__ADMIN_API_KEY=

# AUTH__DEFAULT_TENANT_ID â€” [Optional] Default tenant when X-Tenant-ID not sent. Used by: config.py â†’ AuthSettings.
# AUTH__DEFAULT_TENANT_ID=default

# AUTH__RATE_LIMIT_REQUESTS_PER_MINUTE — [Optional] Per-tenant rate limit; 0 = disable. Used by: config.py; app.py; dashboard.
# Note: /api/v1/health and local dev key `test-key` are exempt from throttling.
# AUTH__RATE_LIMIT_REQUESTS_PER_MINUTE=60

# LLM_INTERNAL__* â€” LLM for internal memory tasks (extraction, consolidation, retrieval). Providers: openai | ollama | anthropic | gemini | vllm | sglang | openai_compatible.
# When USE_LLM_ENABLED=false, modelpack + NER/non-LLM paths are used; no internal LLM calls. Used by: config.py â†’ LLMInternalSettings; llm.py get_internal_llm_client.
# LLM_INTERNAL__PROVIDER=openai
# LLM_INTERNAL__MODEL=gpt-4o-mini
# LLM_INTERNAL__BASE_URL=
# LLM_INTERNAL__API_KEY=
# LLM_STARTUP_VALIDATION_ENABLED=true       # [Optional] Docker startup preflight for LLM endpoint reachability.
# LLM_STARTUP_VALIDATE_IN_CI=false          # [Optional] Keep false to avoid failures in GitHub Actions/CI when endpoint is intentionally absent.
# LLM_STARTUP_VALIDATION_TIMEOUT_SEC=3      # [Optional] Per-endpoint timeout (seconds) for startup validation.
# LLM_EVAL__* â€” [Optional] LLM for evaluation (QA, judge). If unset, LLM_INTERNAL__* is used. Used by: evaluation scripts; llm.py get_eval_llm_client.
# LLM_EVAL__PROVIDER=openai
# LLM_EVAL__MODEL=gpt-4o-mini
# LLM_EVAL__BASE_URL=
# LLM_EVAL__API_KEY=

# EMBEDDING_INTERNAL__* — Embedding for internal memory tasks. Providers: local | openai | openai_compatible | ollama | vllm | mock.
# When unset, defaults to nomic-ai/nomic-embed-text-v2-moe (768 dims, 512 max seq) via sentence-transformers.
# If provider=openai and no API key is configured, CML falls back to deterministic mock embeddings for local/offline runs.
# Embedded mode (py-cml) and examples also read these when not set in code.
# Docker: compose defaults EMBEDDING_INTERNAL__PROVIDER to openai so the API starts without sentence-transformers; set OPENAI_API_KEY (or EMBEDDING_INTERNAL__API_KEY) for embeddings. For local embeddings in Docker, set EMBEDDING_INTERNAL__PROVIDER=local and install sentence-transformers in the image.
# Docker with Ollama on host: EMBEDDING_INTERNAL__PROVIDER=ollama, EMBEDDING_INTERNAL__BASE_URL=http://host.docker.internal:11434/v1 (and LLM_INTERNAL__BASE_URL if used). DIMENSIONS must match DB migration (e.g. 768).
# EMBEDDING_INTERNAL__PROVIDER=local
# EMBEDDING_INTERNAL__MODEL=nomic-ai/nomic-embed-text-v2-moe
# EMBEDDING_INTERNAL__DIMENSIONS=768
# EMBEDDING_INTERNAL__LOCAL_MODEL=nomic-ai/nomic-embed-text-v2-moe
# EMBEDDING_INTERNAL__API_KEY=
# EMBEDDING_INTERNAL__BASE_URL=

# CHUNKER__TOKENIZER â€” [Optional] Hugging Face tokenizer ID for semchunk. Used by: config.py â†’ ChunkerSettings; src/memory/working/manager.py.
# CHUNKER__TOKENIZER=google/flan-t5-base

# CHUNKER__CHUNK_SIZE â€” [Optional] Max tokens per chunk. Used by: config.py; working/manager.py.
# CHUNKER__CHUNK_SIZE=500

# CHUNKER__OVERLAP_PERCENT â€” [Optional] Chunk overlap ratio 0â€“1. Used by: config.py; working/chunker.py.
# CHUNKER__OVERLAP_PERCENT=0.15

# NER__MODEL -- [Optional] spaCy model used by NER fallback paths. Docker image pre-installs en_core_web_sm.
# NER__MODEL=en_core_web_sm
# APP_NAME â€” [Optional] Application name. Used by: config.py â†’ Settings.
# APP_NAME=CognitiveMemoryLayer

# DEBUG â€” [Optional] Enable debug mode (verbose 500 errors). Used by: config.py; routes.py _safe_500_detail.
# DEBUG=false

# CORS_ORIGINS â€” [Optional] Comma-separated origins; unset = default list; DEBUG=true allows "*". Used by: config.py; app.py.
# CORS_ORIGINS=

# Feature flags â€” [Optional] See UsageDocumentation.md.
# FEATURES__USE_LLM_ENABLED=false                # [Optional] Master switch (default false). When false, runtime uses modelpack/NER paths. When true, LLM paths can be enabled per feature (higher latency). Set true for production quality. Excludes Celery jobs.
# FEATURES__STABLE_KEYS_ENABLED=true             # [Optional] SHA256-based stable keys for consolidation facts
# FEATURES__WRITE_TIME_FACTS_ENABLED=true        # [Optional] Populate semantic store at write time
# FEATURES__BATCH_EMBEDDINGS_ENABLED=true        # [Optional] Single embed_batch() per turn
# FEATURES__STORE_ASYNC=false                    # [Optional] Enqueue turn writes to Redis (reduces latency; requires Redis)
# FEATURES__CACHED_EMBEDDINGS_ENABLED=true       # [Optional] Cache embeddings in Redis (requires Redis)
# FEATURES__RETRIEVAL_TIMEOUTS_ENABLED=true      # [Optional] Per-step asyncio.wait_for timeouts (planner.py)
# FEATURES__SKIP_IF_FOUND_CROSS_GROUP=true       # [Optional] Skip remaining steps on fact hit (retriever.py)
# FEATURES__DB_DEPENDENCY_COUNTS=true            # [Optional] DB-side aggregation for forgetting
# FEATURES__BOUNDED_STATE_ENABLED=true           # [Optional] LRU+TTL state maps (working memory)
# FEATURES__HNSW_EF_SEARCH_TUNING=true           # [Optional] Query-time HNSW ef_search (postgres.py)
# FEATURES__CONSTRAINT_EXTRACTION_ENABLED=true   # [Optional] Extract goals, values, policies at write time
# Fine-grained LLM flags (only when USE_LLM_ENABLED=true; otherwise non-LLM modelpack/NER/default paths are used):
# FEATURES__USE_LLM_CONSTRAINT_EXTRACTOR=true    # LLM vs non-LLM constraint extraction path (modelpack/NER/default).
# FEATURES__USE_LLM_WRITE_TIME_FACTS=true        # LLM vs non-LLM write-time facts path (NER/parser).
# FEATURES__USE_LLM_QUERY_CLASSIFIER_ONLY=true   # Bypass modelpack classifier and always use LLM.
# FEATURES__USE_LLM_SALIENCE_REFINEMENT=true     # Use LLM salience from unified extractor vs non-LLM salience/modelpack path.
# FEATURES__USE_LLM_PII_REDACTION=true           # Use LLM PII spans from unified extractor vs non-LLM redaction path.
# FEATURES__USE_LLM_WRITE_GATE_IMPORTANCE=true   # Use LLM importance from unified extractor vs non-LLM write-gate path.
# FEATURES__USE_LLM_MEMORY_TYPE=true             # Use LLM memory_type vs non-LLM mapping/modelpack path.
# FEATURES__USE_LLM_CONFIDENCE=true              # Use LLM confidence when present vs non-LLM default path.
# FEATURES__USE_LLM_CONTEXT_TAGS=true            # Use LLM context_tags when caller does not provide tags.
# FEATURES__USE_LLM_DECAY_RATE=true              # Use LLM decay_rate when valid vs non-LLM default path.
# FEATURES__USE_LLM_CONFLICT_DETECTION_ONLY=true # Bypass non-LLM conflict path and always use LLM conflict detection.
# FEATURES__USE_LLM_CONSTRAINT_RERANKER=true     # LLM to score constraint relevance during reranking (non-LLM uses modelpack when available).

# Retrieval â€” [Optional] Used by: config.py â†’ RetrievalSettings; planner.py, retriever.py, packet_builder.py, postgres.py.
# RETRIEVAL__DEFAULT_STEP_TIMEOUT_MS=5000        # [Optional] Per-step timeout (ms)
# RETRIEVAL__TOTAL_TIMEOUT_MS=15000              # [Optional] Total retrieval budget (ms)
# RETRIEVAL__HNSW_EF_SEARCH=40                   # [Optional] pgvector HNSW ef_search base; max(this, top_k) when tuning on

# =============================================================================
# PART 2 â€” CLIENT & EXAMPLES (py-cml, repo examples, tests)
# =============================================================================
# py-cml and repo examples use these vars only (no fallbacks to other names).
# For local dev, set CML_API_KEY to the same value as server AUTH__API_KEY.
# =============================================================================

# CML_API_KEY â€” [Required] API key (X-API-Key). Used by: py-cml, examples, tests.
CML_API_KEY=test-key

# CML_BASE_URL â€” [Required] CML server base URL. Used by: py-cml, examples, tests.
CML_BASE_URL=http://localhost:8000

# CML_TENANT_ID â€” [Optional] Tenant (X-Tenant-ID). Default: default
# CML_TENANT_ID=default

# CML_TIMEOUT â€” [Optional] Request timeout (seconds). Default: 30.0. Used by examples that need a longer timeout.
# CML_TIMEOUT=30.0

# CML_MAX_RETRIES â€” [Optional] Max retry attempts. Default: 3
# CML_MAX_RETRIES=3

# CML_RETRY_DELAY â€” [Optional] Base delay between retries (seconds). Default: 1.0
# CML_RETRY_DELAY=1.0

# CML_MAX_RETRY_DELAY â€” [Optional] Max backoff delay (seconds). Default: 60.0
# CML_MAX_RETRY_DELAY=60.0

# CML_VERIFY_SSL â€” [Optional] Verify SSL certificates. Default: true
# CML_VERIFY_SSL=true

# CML_ADMIN_API_KEY â€” [Optional] Admin API key for consolidate, run_forgetting, list_tenants.
# CML_ADMIN_API_KEY=

# Embedded mode: EmbeddedCognitiveMemoryLayer reads EMBEDDING_INTERNAL__* and LLM_INTERNAL__* from .env. See PART 1.

# =============================================================================
# PART C â€” OPENAI API KEY
# =============================================================================
# Used when LLM_INTERNAL__API_KEY or EMBEDDING_INTERNAL__API_KEY are unset. Required for OpenAI; optional for local (Ollama).
# OPENAI_API_KEY=your-openai-key

# =============================================================================
# PART D â€” EXAMPLES (Ollama, Anthropic)
# =============================================================================
# OLLAMA_BASE_URL â€” [Optional] Ollama API URL for Ollama examples.
# OLLAMA_BASE_URL=http://localhost:11434/v1
# ANTHROPIC_API_KEY â€” [Optional] Used by examples/tool-calling.
# ANTHROPIC_API_KEY=

# =============================================================================
# PART E â€” DEVELOPMENT & TESTING
# =============================================================================
# Set AUTH__API_KEY, CML_API_KEY, CML_BASE_URL, and DB URLs so tests run without skipping.
# =============================================================================

# USE_ENV_DB â€” [Optional] Use existing Postgres instead of testcontainers. Used by: server integration tests.
# USE_ENV_DB=1

# CML_TEST_URL â€” [Optional] Override CML_BASE_URL for py-cml integration/e2e tests.
# CML_TEST_URL=http://localhost:8000
# CML_TEST_API_KEY â€” [Optional] Override CML_API_KEY for py-cml integration/e2e tests.
# CML_TEST_API_KEY=
