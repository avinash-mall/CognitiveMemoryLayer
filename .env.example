# =============================================================================
# Cognitive Memory Layer (CML) — Environment Configuration
# =============================================================================
# Copy to .env and set values. [Required] = must set. [Optional but Highly Recommended] = crucial for full functionality. [Optional] = has default.
# Tests (server and py-cml) read from .env; no hardcoded fallbacks. Set required
# vars below so tests and examples run without skipping.
# =============================================================================

# ---------- PACKAGE VERSION (build) ----------
# VERSION — [Optional] Package version for Hatch build. Used by: pyproject.toml (dynamic version). Fallback: VERSION env or repo VERSION file. Bump when releasing.
# VERSION=1.3.2

# =============================================================================
# PART 1 — CML SERVER (API) — from src/core/config.py
# =============================================================================
# Server reads settings via pydantic-settings. Nested keys use double
# underscore: SECTION__KEY (e.g. DATABASE__POSTGRES_URL). Values match
# docker-compose for local development where relevant.
# =============================================================================

# ---------- REQUIRED ----------

# AUTH__API_KEY — [Required] API key for memory operations (X-API-Key header).
# Used by: src/core/config.py → AuthSettings.api_key; src/api/auth.py.
# Use test-key for local dev (py-cml tests); set strong secret in production.
AUTH__API_KEY=test-key

# DATABASE__POSTGRES_URL — [Required] PostgreSQL connection string.
# Used by: src/core/config.py → DatabaseSettings.postgres_url; src/storage/connection.py.
# Format: postgresql+asyncpg://user:password@host:port/dbname
DATABASE__POSTGRES_URL=postgresql+asyncpg://memory:memory@localhost/memory

# ---------- OPTIONAL BUT HIGHLY RECOMMENDED ----------

# DATABASE__NEO4J_URL — [Optional but Highly Recommended] Neo4j Bolt URL for knowledge graph. Used by: config.py → DatabaseSettings; src/storage/neo4j.py. Required for semantic/graph features.
# DATABASE__NEO4J_URL=bolt://localhost:7687

# DATABASE__NEO4J_USER — [Optional but Highly Recommended] Neo4j username. Used by: config.py → DatabaseSettings; src/storage/neo4j.py.
# DATABASE__NEO4J_USER=neo4j

# DATABASE__NEO4J_PASSWORD — [Optional but Highly Recommended] Neo4j password. Set when Neo4j is secured. Default: empty. Used by: config.py; neo4j.py. Required for non-localhost Neo4j.
# DATABASE__NEO4J_PASSWORD=

# DATABASE__NEO4J_BROWSER_URL — [Optional] Neo4j bolt URL for dashboard graph (neovis.js). When the API runs in Docker (neo4j:7687) but the browser is on the host, set to bolt://localhost:7687. Default: same as DATABASE__NEO4J_URL.
# DATABASE__NEO4J_BROWSER_URL=bolt://localhost:7687

# DATABASE__REDIS_URL — [Optional but Highly Recommended] Redis connection for cache, sessions, rate limits, embedding cache. Used by: config.py; src/storage/redis.py; celery_app.py. Required for CACHED_EMBEDDINGS_ENABLED, STORE_ASYNC, sessions.
# DATABASE__REDIS_URL=redis://localhost:6379

# ---------- OPTIONAL (defaults shown) ----------

# AUTH__ADMIN_API_KEY — [Optional] Admin API key for dashboard, consolidate, run_forgetting, list_tenants. Default: None. Used by: config.py; auth.py.
# AUTH__ADMIN_API_KEY=

# AUTH__DEFAULT_TENANT_ID — [Optional] Default tenant when X-Tenant-ID not sent. Used by: config.py → AuthSettings.
# AUTH__DEFAULT_TENANT_ID=default

# AUTH__RATE_LIMIT_REQUESTS_PER_MINUTE — [Optional] Per-tenant rate limit; 0 = disable. Used by: config.py; app.py; dashboard_routes.py.
# AUTH__RATE_LIMIT_REQUESTS_PER_MINUTE=60

# LLM_INTERNAL__* — LLM for internal memory tasks (extraction, consolidation, retrieval). Providers: openai | ollama | anthropic | gemini | vllm | sglang | openai_compatible.
# When USE_LLM_ENABLED=false, heuristics are used; no LLM calls. Used by: config.py → LLMInternalSettings; llm.py get_internal_llm_client.
# LLM_INTERNAL__PROVIDER=openai
# LLM_INTERNAL__MODEL=gpt-4o-mini
# LLM_INTERNAL__BASE_URL=
# LLM_INTERNAL__API_KEY=

# LLM_EVAL__* — [Optional] LLM for evaluation (QA, judge). If unset, LLM_INTERNAL__* is used. Used by: evaluation scripts; llm.py get_eval_llm_client.
# LLM_EVAL__PROVIDER=openai
# LLM_EVAL__MODEL=gpt-4o-mini
# LLM_EVAL__BASE_URL=
# LLM_EVAL__API_KEY=

# EMBEDDING_INTERNAL__* — Embedding for internal memory tasks. When unset, defaults to nomic-ai/nomic-embed-text-v2-moe (768 dims, 512 max seq) via sentence-transformers.
# EMBEDDING_INTERNAL__PROVIDER=local
# EMBEDDING_INTERNAL__MODEL=nomic-ai/nomic-embed-text-v2-moe
# EMBEDDING_INTERNAL__DIMENSIONS=768
# EMBEDDING_INTERNAL__LOCAL_MODEL=nomic-ai/nomic-embed-text-v2-moe
# EMBEDDING_INTERNAL__API_KEY=
# EMBEDDING_INTERNAL__BASE_URL=

# CHUNKER__TOKENIZER — [Optional] Hugging Face tokenizer ID for semchunk. Used by: config.py → ChunkerSettings; src/memory/working/manager.py.
# CHUNKER__TOKENIZER=google/flan-t5-base

# CHUNKER__CHUNK_SIZE — [Optional] Max tokens per chunk. Used by: config.py; working/manager.py.
# CHUNKER__CHUNK_SIZE=500

# CHUNKER__OVERLAP_PERCENT — [Optional] Chunk overlap ratio 0–1. Used by: config.py; working/chunker.py.
# CHUNKER__OVERLAP_PERCENT=0.15

# APP_NAME — [Optional] Application name. Used by: config.py → Settings.
# APP_NAME=CognitiveMemoryLayer

# DEBUG — [Optional] Enable debug mode (verbose 500 errors). Used by: config.py; routes.py _safe_500_detail.
# DEBUG=false

# CORS_ORIGINS — [Optional] Comma-separated origins; unset = default list; DEBUG=true allows "*". Used by: config.py; app.py.
# CORS_ORIGINS=

# Feature flags — [Optional] See UsageDocumentation.md.
# FEATURES__USE_LLM_ENABLED=false                # [Optional] Master switch (default false). WARNING: Quality degrades when false (heuristics used). Enabling improves quality but lowers performance (LLM latency). Set true for production quality. Excludes Celery jobs.
# FEATURES__STABLE_KEYS_ENABLED=true             # [Optional] SHA256-based stable keys for consolidation facts
# FEATURES__WRITE_TIME_FACTS_ENABLED=true        # [Optional] Populate semantic store at write time
# FEATURES__BATCH_EMBEDDINGS_ENABLED=true        # [Optional] Single embed_batch() per turn
# FEATURES__STORE_ASYNC=false                    # [Optional] Enqueue turn writes to Redis (reduces latency; requires Redis)
# FEATURES__CACHED_EMBEDDINGS_ENABLED=true       # [Optional] Cache embeddings in Redis (requires Redis)
# FEATURES__RETRIEVAL_TIMEOUTS_ENABLED=true      # [Optional] Per-step asyncio.wait_for timeouts (planner.py)
# FEATURES__SKIP_IF_FOUND_CROSS_GROUP=true       # [Optional] Skip remaining steps on fact hit (retriever.py)
# FEATURES__DB_DEPENDENCY_COUNTS=true            # [Optional] DB-side aggregation for forgetting
# FEATURES__BOUNDED_STATE_ENABLED=true           # [Optional] LRU+TTL state maps (working memory)
# FEATURES__HNSW_EF_SEARCH_TUNING=true           # [Optional] Query-time HNSW ef_search (postgres.py)
# FEATURES__CONSTRAINT_EXTRACTION_ENABLED=true   # [Optional] Extract goals, values, policies at write time
# Fine-grained LLM flags (only when USE_LLM_ENABLED=true; ignored otherwise — heuristic always used):
# FEATURES__USE_LLM_CONSTRAINT_EXTRACTOR=true    # LLM vs rule-based ConstraintExtractor. Heuristic: rule-based.
# FEATURES__USE_LLM_WRITE_TIME_FACTS=true        # LLM vs rule-based WriteTimeFactExtractor. Heuristic: rule-based.
# FEATURES__USE_LLM_QUERY_CLASSIFIER_ONLY=true   # Skip fast pattern path; always use LLM. Heuristic: pattern-based _fast_classify.
# FEATURES__USE_LLM_SALIENCE_REFINEMENT=true     # Unified extractor salience. Heuristic: _compute_importance.
# FEATURES__USE_LLM_PII_REDACTION=true           # Unified extractor PII spans. Heuristic: regex _check_pii.
# FEATURES__USE_LLM_WRITE_GATE_IMPORTANCE=true   # Unified extractor importance. Heuristic: _compute_importance.
# FEATURES__USE_LLM_MEMORY_TYPE=true             # Unified extractor memory_type. Heuristic: ChunkType mapping.
# FEATURES__USE_LLM_CONFIDENCE=true              # Unified extractor confidence. Heuristic: default 0.5.
# FEATURES__USE_LLM_CONTEXT_TAGS=true            # Unified extractor context_tags. Heuristic: caller-provided or empty.
# FEATURES__USE_LLM_DECAY_RATE=true              # Unified extractor decay_rate. Heuristic: default 0.01.
# FEATURES__USE_LLM_CONFLICT_DETECTION_ONLY=true # Skip fast path for conflict detection. Heuristic: _fast_detect patterns.
# FEATURES__USE_LLM_CONSTRAINT_RERANKER=true     # LLM to score constraint relevance during reranking. Heuristic: text-similarity.

# Retrieval — [Optional] Used by: config.py → RetrievalSettings; planner.py, retriever.py, packet_builder.py, postgres.py.
# RETRIEVAL__DEFAULT_STEP_TIMEOUT_MS=5000        # [Optional] Per-step timeout (ms)
# RETRIEVAL__TOTAL_TIMEOUT_MS=15000              # [Optional] Total retrieval budget (ms)
# RETRIEVAL__HNSW_EF_SEARCH=40                   # [Optional] pgvector HNSW ef_search base; max(this, top_k) when tuning on

# =============================================================================
# PART 2 — PY-CML CLIENT (packages/py-cml)
# =============================================================================
# Used when connecting to the CML API from Python. Loaded from env or .env;
# constructor args override env.
# =============================================================================

# ---------- REQUIRED ----------

# CML_API_KEY — [Required] API key (sent as X-API-Key). Used by: py-cml config.py → CMLConfig; client requests. Use test-key for local dev.
CML_API_KEY=test-key

# CML_BASE_URL — [Required] CML server base URL. Used by: py-cml config.py; no default in code.
CML_BASE_URL=http://localhost:8000

# ---------- OPTIONAL (defaults shown) ----------

# CML_TENANT_ID — [Optional] Tenant identifier (X-Tenant-ID). Used by: py-cml config.py. Default: default
# CML_TENANT_ID=default

# CML_TIMEOUT — [Optional] Request timeout (seconds). Used by: py-cml config.py; client requests. Default: 30.0
# CML_TIMEOUT=30.0

# CML_MAX_RETRIES — [Optional] Max retry attempts. Used by: py-cml config.py; client requests. Default: 3
# CML_MAX_RETRIES=3

# CML_RETRY_DELAY — [Optional] Base delay between retries (seconds). Used by: py-cml config.py; exponential backoff. Default: 1.0
# CML_RETRY_DELAY=1.0

# CML_MAX_RETRY_DELAY — [Optional] Max backoff delay (seconds). Used by: py-cml config.py; caps exponential backoff. Default: 60.0
# CML_MAX_RETRY_DELAY=60.0

# CML_VERIFY_SSL — [Optional] Verify SSL certificates. Used by: py-cml config.py; client requests. Default: true
# CML_VERIFY_SSL=true

# CML_ADMIN_API_KEY — [Optional] Admin API key for consolidate, run_forgetting, list_tenants. Used by: py-cml config.py.
# CML_ADMIN_API_KEY=

# ---------- EMBEDDED MODE [Optional] ----------
# EmbeddedCognitiveMemoryLayer reads these from .env when not set in code.
# EMBEDDING_INTERNAL__MODEL — [Optional] Model for local embeddings. Default nomic-ai/nomic-embed-text-v2-moe.
# EMBEDDING_INTERNAL__DIMENSIONS — [Optional] Vector dimension; default 768; should match server when syncing.
# EMBEDDING_INTERNAL__BASE_URL — [Optional] OpenAI-compatible embedding endpoint.
# LLM_INTERNAL__MODEL — [Optional] Model for embedded LLM.
# LLM_INTERNAL__BASE_URL — [Optional] LLM endpoint for embedded mode.

# =============================================================================
# PART C — FALLBACK API KEYS
# =============================================================================
# Fallback when LLM_INTERNAL__API_KEY or EMBEDDING_INTERNAL__API_KEY are unset. Used by: src/utils/llm.py; src/utils/embeddings.py.
# =============================================================================

# OPENAI_API_KEY — [REQUIRED when using OpenAI for LLM or embeddings] Fallback for LLM_INTERNAL__API_KEY, EMBEDDING_INTERNAL__API_KEY when unset. Required for OpenAI; optional for local (Ollama).
# OPENAI_API_KEY=your-openai-key

# =============================================================================
# PART D — EXAMPLES & SCRIPTS
# =============================================================================
# Used by repo examples/ and tests; not part of core server or py-cml config.
# =============================================================================

# MEMORY_API_URL — [Optional] CML API URL for repo examples. Used by: examples; fallback for py-cml CML_BASE_URL (config.py env_map).
# MEMORY_API_URL=http://localhost:8000

# MEMORY_API_KEY — [Optional] Examples use this or AUTH__API_KEY.
# MEMORY_API_KEY=

# MEMORY_API_TIMEOUT — [Optional] Timeout for examples (e.g. ollama_chat_test.py).
# MEMORY_API_TIMEOUT=120

# OLLAMA_BASE_URL — [Optional] Ollama API URL for Ollama examples. Used by: examples.
# OLLAMA_BASE_URL=http://localhost:11434/v1

# ANTHROPIC_API_KEY — [Optional] Used by examples/tool-calling.
# ANTHROPIC_API_KEY=

# =============================================================================
# PART F — DEVELOPMENT & TESTING
# =============================================================================
# Server and py-cml tests read all variables from this file (no hardcoded fallbacks).
# Set AUTH__API_KEY, EMBEDDING_INTERNAL__DIMENSIONS, and DB URLs so tests run without skipping.
# =============================================================================

# USE_ENV_DB — [Optional] Use existing Postgres instead of testcontainers. Used by: server integration tests.
# USE_ENV_DB=1

# py-cml integration/e2e — tests load repo root .env; use CML_BASE_URL or CML_TEST_URL, CML_API_KEY or CML_TEST_API_KEY. Use same AUTH__API_KEY as the server (e.g. test-key).
# CML_TEST_URL — [Optional] Override CML_BASE_URL for py-cml tests. Used by: tests/conftest.
# CML_TEST_URL=http://localhost:8000
# CML_TEST_API_KEY — [Optional] Override CML_API_KEY for py-cml tests; falls back to AUTH__API_KEY.
# CML_TEST_API_KEY=
