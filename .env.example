# =============================================================================
# Cognitive Memory Layer (CML) — Environment Configuration
# =============================================================================
# Copy to .env and set values. [Required] = must set. [Optional] = has default.
# Migration: If you used MEMORY_API_URL, MEMORY_API_KEY, or MEMORY_API_TIMEOUT, set CML_BASE_URL, CML_API_KEY, and CML_TIMEOUT instead (no fallbacks).
# =============================================================================

# ---------- PACKAGE VERSION (build) ----------
# VERSION — [Optional] Package version for Hatch build. Used by: pyproject.toml (dynamic version). Fallback: VERSION env or repo VERSION file. Bump when releasing.
# VERSION=1.3.2

# =============================================================================
# PART 1 — CML SERVER (API) — from src/core/config.py
# =============================================================================
# Server reads settings via pydantic-settings. Nested keys use double
# underscore: SECTION__KEY (e.g. DATABASE__POSTGRES_URL). Values match
# docker-compose for local development where relevant.
# =============================================================================

# ---------- Database ----------
# Docker: set POSTGRES_*, NEO4J_USER, NEO4J_PASSWORD; compose injects DATABASE__* for the app. Local: set only DATABASE__* below (host localhost).
# POSTGRES_USER=memory
# POSTGRES_PASSWORD=memory
# POSTGRES_DB=memory
# NEO4J_USER=neo4j
# NEO4J_PASSWORD=password

# App (config.py → DatabaseSettings). Required.
DATABASE__POSTGRES_URL=postgresql+asyncpg://memory:memory@localhost/memory

# ---------- REQUIRED ----------

# AUTH__API_KEY — [Required] API key for memory operations (X-API-Key header). Used by: config.py → AuthSettings; src/api/auth.py. Use test-key for local dev; set strong secret in production. To run the API with test keys without editing .env, use docker compose -f docker/docker-compose.yml -f docker/docker-compose.test-key.yml up api.
AUTH__API_KEY=test-key

# ---------- OPTIONAL BUT HIGHLY RECOMMENDED ----------

# DATABASE__NEO4J_URL — [Optional] Neo4j Bolt URL. Used by: config.py; src/storage/neo4j.py. Local: bolt://localhost:7687. Docker: compose overrides bolt://neo4j:7687.
# DATABASE__NEO4J_URL=bolt://localhost:7687

# DATABASE__NEO4J_USER — [Optional] Neo4j username. Docker: compose overrides from NEO4J_USER.
# DATABASE__NEO4J_USER=neo4j

# DATABASE__NEO4J_PASSWORD — [Optional] Neo4j password. Docker: compose overrides from NEO4J_PASSWORD.
# DATABASE__NEO4J_PASSWORD=password

# DATABASE__NEO4J_BROWSER_URL — [Optional] Neo4j Bolt URL for dashboard (neovis.js). When API in Docker, browser on host: bolt://localhost:7687. Else same as DATABASE__NEO4J_URL.
# DATABASE__NEO4J_BROWSER_URL=bolt://localhost:7687

# DATABASE__REDIS_URL — [Optional] Redis URL. Used by: config.py; src/storage/redis.py. Local: redis://localhost:6379. Docker: compose overrides redis://redis:6379.
# DATABASE__REDIS_URL=redis://localhost:6379

# ---------- OPTIONAL (defaults shown) ----------

# AUTH__ADMIN_API_KEY — [Optional] Admin API key for dashboard, consolidate, run_forgetting, list_tenants. Default: None. Used by: config.py; auth.py.
# AUTH__ADMIN_API_KEY=

# AUTH__DEFAULT_TENANT_ID — [Optional] Default tenant when X-Tenant-ID not sent. Used by: config.py → AuthSettings.
# AUTH__DEFAULT_TENANT_ID=default

# AUTH__RATE_LIMIT_REQUESTS_PER_MINUTE — [Optional] Per-tenant rate limit; 0 = disable. Used by: config.py; app.py; dashboard_routes.py.
# AUTH__RATE_LIMIT_REQUESTS_PER_MINUTE=60

# LLM_INTERNAL__* — LLM for internal memory tasks (extraction, consolidation, retrieval). Providers: openai | ollama | anthropic | gemini | vllm | sglang | openai_compatible.
# When USE_LLM_ENABLED=false, heuristics are used; no LLM calls. Used by: config.py → LLMInternalSettings; llm.py get_internal_llm_client.
# LLM_INTERNAL__PROVIDER=openai
# LLM_INTERNAL__MODEL=gpt-4o-mini
# LLM_INTERNAL__BASE_URL=
# LLM_INTERNAL__API_KEY=

# LLM_EVAL__* — [Optional] LLM for evaluation (QA, judge). If unset, LLM_INTERNAL__* is used. Used by: evaluation scripts; llm.py get_eval_llm_client.
# LLM_EVAL__PROVIDER=openai
# LLM_EVAL__MODEL=gpt-4o-mini
# LLM_EVAL__BASE_URL=
# LLM_EVAL__API_KEY=

# EMBEDDING_INTERNAL__* — Embedding for internal memory tasks. When unset, defaults to nomic-ai/nomic-embed-text-v2-moe (768 dims, 512 max seq) via sentence-transformers. Embedded mode (py-cml) and examples also read these when not set in code.
# Docker: compose defaults EMBEDDING_INTERNAL__PROVIDER to openai so the API starts without sentence-transformers; set OPENAI_API_KEY (or EMBEDDING_INTERNAL__API_KEY) for embeddings. For local embeddings in Docker, set EMBEDDING_INTERNAL__PROVIDER=local and install sentence-transformers in the image.
# Docker with Ollama on host: EMBEDDING_INTERNAL__PROVIDER=ollama, EMBEDDING_INTERNAL__BASE_URL=http://host.docker.internal:11434/v1 (and LLM_INTERNAL__BASE_URL if used). DIMENSIONS must match DB migration (e.g. 768).
# EMBEDDING_INTERNAL__PROVIDER=local
# EMBEDDING_INTERNAL__MODEL=nomic-ai/nomic-embed-text-v2-moe
# EMBEDDING_INTERNAL__DIMENSIONS=768
# EMBEDDING_INTERNAL__LOCAL_MODEL=nomic-ai/nomic-embed-text-v2-moe
# EMBEDDING_INTERNAL__API_KEY=
# EMBEDDING_INTERNAL__BASE_URL=

# CHUNKER__TOKENIZER — [Optional] Hugging Face tokenizer ID for semchunk. Used by: config.py → ChunkerSettings; src/memory/working/manager.py.
# CHUNKER__TOKENIZER=google/flan-t5-base

# CHUNKER__CHUNK_SIZE — [Optional] Max tokens per chunk. Used by: config.py; working/manager.py.
# CHUNKER__CHUNK_SIZE=500

# CHUNKER__OVERLAP_PERCENT — [Optional] Chunk overlap ratio 0–1. Used by: config.py; working/chunker.py.
# CHUNKER__OVERLAP_PERCENT=0.15

# APP_NAME — [Optional] Application name. Used by: config.py → Settings.
# APP_NAME=CognitiveMemoryLayer

# DEBUG — [Optional] Enable debug mode (verbose 500 errors). Used by: config.py; routes.py _safe_500_detail.
# DEBUG=false

# CORS_ORIGINS — [Optional] Comma-separated origins; unset = default list; DEBUG=true allows "*". Used by: config.py; app.py.
# CORS_ORIGINS=

# Feature flags — [Optional] See UsageDocumentation.md.
# FEATURES__USE_LLM_ENABLED=false                # [Optional] Master switch (default false). WARNING: Quality degrades when false (heuristics used). Enabling improves quality but lowers performance (LLM latency). Set true for production quality. Excludes Celery jobs.
# FEATURES__STABLE_KEYS_ENABLED=true             # [Optional] SHA256-based stable keys for consolidation facts
# FEATURES__WRITE_TIME_FACTS_ENABLED=true        # [Optional] Populate semantic store at write time
# FEATURES__BATCH_EMBEDDINGS_ENABLED=true        # [Optional] Single embed_batch() per turn
# FEATURES__STORE_ASYNC=false                    # [Optional] Enqueue turn writes to Redis (reduces latency; requires Redis)
# FEATURES__CACHED_EMBEDDINGS_ENABLED=true       # [Optional] Cache embeddings in Redis (requires Redis)
# FEATURES__RETRIEVAL_TIMEOUTS_ENABLED=true      # [Optional] Per-step asyncio.wait_for timeouts (planner.py)
# FEATURES__SKIP_IF_FOUND_CROSS_GROUP=true       # [Optional] Skip remaining steps on fact hit (retriever.py)
# FEATURES__DB_DEPENDENCY_COUNTS=true            # [Optional] DB-side aggregation for forgetting
# FEATURES__BOUNDED_STATE_ENABLED=true           # [Optional] LRU+TTL state maps (working memory)
# FEATURES__HNSW_EF_SEARCH_TUNING=true           # [Optional] Query-time HNSW ef_search (postgres.py)
# FEATURES__CONSTRAINT_EXTRACTION_ENABLED=true   # [Optional] Extract goals, values, policies at write time
# Fine-grained LLM flags (only when USE_LLM_ENABLED=true; ignored otherwise — heuristic always used):
# FEATURES__USE_LLM_CONSTRAINT_EXTRACTOR=true    # LLM vs rule-based ConstraintExtractor. Heuristic: rule-based.
# FEATURES__USE_LLM_WRITE_TIME_FACTS=true        # LLM vs rule-based WriteTimeFactExtractor. Heuristic: rule-based.
# FEATURES__USE_LLM_QUERY_CLASSIFIER_ONLY=true   # Skip fast pattern path; always use LLM. Heuristic: pattern-based _fast_classify.
# FEATURES__USE_LLM_SALIENCE_REFINEMENT=true     # Unified extractor salience. Heuristic: _compute_importance.
# FEATURES__USE_LLM_PII_REDACTION=true           # Unified extractor PII spans. Heuristic: regex _check_pii.
# FEATURES__USE_LLM_WRITE_GATE_IMPORTANCE=true   # Unified extractor importance. Heuristic: _compute_importance.
# FEATURES__USE_LLM_MEMORY_TYPE=true             # Unified extractor memory_type. Heuristic: ChunkType mapping.
# FEATURES__USE_LLM_CONFIDENCE=true              # Unified extractor confidence. Heuristic: default 0.5.
# FEATURES__USE_LLM_CONTEXT_TAGS=true            # Unified extractor context_tags. Heuristic: caller-provided or empty.
# FEATURES__USE_LLM_DECAY_RATE=true              # Unified extractor decay_rate. Heuristic: default 0.01.
# FEATURES__USE_LLM_CONFLICT_DETECTION_ONLY=true # Skip fast path for conflict detection. Heuristic: _fast_detect patterns.
# FEATURES__USE_LLM_CONSTRAINT_RERANKER=true     # LLM to score constraint relevance during reranking. Heuristic: text-similarity.

# Retrieval — [Optional] Used by: config.py → RetrievalSettings; planner.py, retriever.py, packet_builder.py, postgres.py.
# RETRIEVAL__DEFAULT_STEP_TIMEOUT_MS=5000        # [Optional] Per-step timeout (ms)
# RETRIEVAL__TOTAL_TIMEOUT_MS=15000              # [Optional] Total retrieval budget (ms)
# RETRIEVAL__HNSW_EF_SEARCH=40                   # [Optional] pgvector HNSW ef_search base; max(this, top_k) when tuning on

# =============================================================================
# PART 2 — CLIENT & EXAMPLES (py-cml, repo examples, tests)
# =============================================================================
# py-cml and repo examples use these vars only (no fallbacks to other names).
# For local dev, set CML_API_KEY to the same value as server AUTH__API_KEY.
# =============================================================================

# CML_API_KEY — [Required] API key (X-API-Key). Used by: py-cml, examples, tests.
CML_API_KEY=test-key

# CML_BASE_URL — [Required] CML server base URL. Used by: py-cml, examples, tests.
CML_BASE_URL=http://localhost:8000

# CML_TENANT_ID — [Optional] Tenant (X-Tenant-ID). Default: default
# CML_TENANT_ID=default

# CML_TIMEOUT — [Optional] Request timeout (seconds). Default: 30.0. Used by examples that need a longer timeout.
# CML_TIMEOUT=30.0

# CML_MAX_RETRIES — [Optional] Max retry attempts. Default: 3
# CML_MAX_RETRIES=3

# CML_RETRY_DELAY — [Optional] Base delay between retries (seconds). Default: 1.0
# CML_RETRY_DELAY=1.0

# CML_MAX_RETRY_DELAY — [Optional] Max backoff delay (seconds). Default: 60.0
# CML_MAX_RETRY_DELAY=60.0

# CML_VERIFY_SSL — [Optional] Verify SSL certificates. Default: true
# CML_VERIFY_SSL=true

# CML_ADMIN_API_KEY — [Optional] Admin API key for consolidate, run_forgetting, list_tenants.
# CML_ADMIN_API_KEY=

# Embedded mode: EmbeddedCognitiveMemoryLayer reads EMBEDDING_INTERNAL__* and LLM_INTERNAL__* from .env. See PART 1.

# =============================================================================
# PART C — OPENAI API KEY
# =============================================================================
# Used when LLM_INTERNAL__API_KEY or EMBEDDING_INTERNAL__API_KEY are unset. Required for OpenAI; optional for local (Ollama).
# OPENAI_API_KEY=your-openai-key

# =============================================================================
# PART D — EXAMPLES (Ollama, Anthropic)
# =============================================================================
# OLLAMA_BASE_URL — [Optional] Ollama API URL for Ollama examples.
# OLLAMA_BASE_URL=http://localhost:11434/v1
# ANTHROPIC_API_KEY — [Optional] Used by examples/tool-calling.
# ANTHROPIC_API_KEY=

# =============================================================================
# PART E — DEVELOPMENT & TESTING
# =============================================================================
# Set AUTH__API_KEY, CML_API_KEY, CML_BASE_URL, and DB URLs so tests run without skipping.
# =============================================================================

# USE_ENV_DB — [Optional] Use existing Postgres instead of testcontainers. Used by: server integration tests.
# USE_ENV_DB=1

# CML_TEST_URL — [Optional] Override CML_BASE_URL for py-cml integration/e2e tests.
# CML_TEST_URL=http://localhost:8000
# CML_TEST_API_KEY — [Optional] Override CML_API_KEY for py-cml integration/e2e tests.
# CML_TEST_API_KEY=
