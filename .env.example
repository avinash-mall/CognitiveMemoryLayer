# =============================================================================
# Cognitive Memory Layer (CML) — Environment Configuration
# =============================================================================
# Copy to .env and set values. [Required] = must set. [Optional] = has default.
# Tests (server and py-cml) read from .env; no hardcoded fallbacks. Set required
# vars below so tests and examples run without skipping.
# =============================================================================

# ---------- PACKAGE VERSION (build) ----------
# Used by Hatch at build time (pyproject.toml dynamic version). Fallback: env
# VERSION or repo root VERSION file. Bump when releasing.
VERSION=1.1.0

# =============================================================================
# PART 1 — CML SERVER (API) — from src/core/config.py
# =============================================================================
# Server reads settings via pydantic-settings. Nested keys use double
# underscore: SECTION__KEY (e.g. DATABASE__POSTGRES_URL). Values match
# docker-compose for local development where relevant.
# =============================================================================

# ---------- REQUIRED ----------

# [Required] API key for memory operations (X-API-Key header). Use test-key for
# local dev so py-cml integration/e2e tests work; set strong secret in production.
AUTH__API_KEY=test-key

# [Required] PostgreSQL connection. Format: postgresql+asyncpg://user:password@host:port/dbname
DATABASE__POSTGRES_URL=postgresql+asyncpg://memory:memory@localhost/memory

# ---------- OPTIONAL (defaults shown) ----------

# Database (defaults from config.py)
DATABASE__NEO4J_URL=bolt://localhost:7687
DATABASE__NEO4J_USER=neo4j
# Set when Neo4j URL is not localhost (e.g. cloud or secured instance). Default: empty
# DATABASE__NEO4J_PASSWORD=
DATABASE__REDIS_URL=redis://localhost:6379

# Auth (for dashboard 403 tests, set AUTH__ADMIN_API_KEY to a different value than AUTH__API_KEY)
# AUTH__ADMIN_API_KEY=   (default: None; for dashboard, consolidate, run_forgetting, list_tenants)
AUTH__DEFAULT_TENANT_ID=default
AUTH__RATE_LIMIT_REQUESTS_PER_MINUTE=60

# LLM — provider: openai | openai_compatible | ollama | gemini | claude
LLM__PROVIDER=openai
LLM__MODEL=gpt-4o-mini
# LLM__API_KEY=   (default: None; server can use OPENAI_API_KEY)
# LLM__BASE_URL=   (default: None; for openai_compatible / Ollama / proxy)
# Example — Ollama: LLM__PROVIDER=ollama  LLM__MODEL=llama3.2  LLM__BASE_URL=http://localhost:11434/v1

# Embedding — provider: openai | local | openai_compatible | ollama
EMBEDDING__PROVIDER=openai
EMBEDDING__MODEL=text-embedding-3-small
# Vector dimension: must match the embedding model output and DB schema.
# Server, migrations, and tests all read this from .env (Docker does not override it).
EMBEDDING__DIMENSIONS=1536
EMBEDDING__LOCAL_MODEL=all-MiniLM-L6-v2
# EMBEDDING__API_KEY=   (default: None; server can use OPENAI_API_KEY)
# EMBEDDING__BASE_URL=   (default: None; OpenAI-compatible embedding endpoint)
# If you change EMBEDDING__DIMENSIONS, run: docker compose -f docker/docker-compose.yml down -v
# then re-run migrations (e.g. up api or run --rm app).

# Application (top-level)
APP_NAME=CognitiveMemoryLayer
DEBUG=false
# CORS_ORIGINS=   (default: None; unset = default list; DEBUG=true allows "*")

# Feature flags (optional; all default true except STORE_ASYNC). See UsageDocumentation.md § Configuration Reference.
# FEATURES__STABLE_KEYS_ENABLED=true
# FEATURES__WRITE_TIME_FACTS_ENABLED=true
# FEATURES__BATCH_EMBEDDINGS_ENABLED=true
# FEATURES__STORE_ASYNC=false
# FEATURES__CACHED_EMBEDDINGS_ENABLED=true
# FEATURES__RETRIEVAL_TIMEOUTS_ENABLED=true
# FEATURES__SKIP_IF_FOUND_CROSS_GROUP=true
# FEATURES__DB_DEPENDENCY_COUNTS=true
# FEATURES__BOUNDED_STATE_ENABLED=true
# FEATURES__HNSW_EF_SEARCH_TUNING=true
# FEATURES__CONSTRAINT_EXTRACTION_ENABLED=true   # Extract cognitive constraints (goals, values, policies) at write time

# Retrieval (timeouts and pgvector tuning)
# RETRIEVAL__DEFAULT_STEP_TIMEOUT_MS=5000
# RETRIEVAL__TOTAL_TIMEOUT_MS=15000
# RETRIEVAL__HNSW_EF_SEARCH=40

# =============================================================================
# PART 2 — PY-CML CLIENT (packages/py-cml)
# =============================================================================
# Used when connecting to the CML API from Python. Loaded from env or .env;
# constructor args override env.
# =============================================================================

# ---------- REQUIRED ----------

# [Required] API key (sent as X-API-Key). Use test-key for local dev so tests pass.
CML_API_KEY=test-key

# [Required] CML server base URL (no default in code).
CML_BASE_URL=http://localhost:8000

# ---------- OPTIONAL (defaults shown) ----------

CML_TENANT_ID=default
CML_TIMEOUT=30.0
CML_MAX_RETRIES=3
CML_RETRY_DELAY=1.0
CML_MAX_RETRY_DELAY=60.0
CML_VERIFY_SSL=true
# CML_ADMIN_API_KEY=   (optional; for consolidate, run_forgetting, list_tenants)

# ---------- EMBEDDED MODE (optional only) ----------
# EmbeddedCognitiveMemoryLayer reads these from .env when not set in code.
# EMBEDDING__MODEL=
# EMBEDDING__DIMENSIONS=   (default 384 if unset; should match server when syncing with API)
# EMBEDDING__BASE_URL=
# LLM__MODEL=
# LLM__BASE_URL=

# =============================================================================
# PART C — FALLBACK API KEYS
# =============================================================================
# Used by server when LLM__API_KEY or EMBEDDING__API_KEY are unset (e.g. OpenAI).
# =============================================================================

# OPENAI_API_KEY=your-openai-key

# =============================================================================
# PART D — EXAMPLES & SCRIPTS
# =============================================================================
# Used by repo examples/ and tests; not part of core server or py-cml config.
# =============================================================================

# [Required for repo examples] CML API URL (examples use MEMORY_API_URL or CML_BASE_URL)
MEMORY_API_URL=http://localhost:8000

# MEMORY_API_KEY=   (optional; often same as AUTH__API_KEY)
# MEMORY_API_TIMEOUT=120   (optional; e.g. ollama_chat_test.py)

# [Required for Ollama examples] Ollama base URL
OLLAMA_BASE_URL=http://localhost:11434/v1

# Model name for examples: LLM__MODEL (Part 1) or OPENAI_MODEL
# ANTHROPIC_API_KEY=   (optional; examples/tool-calling)

# =============================================================================
# PART F — DEVELOPMENT & TESTING
# =============================================================================
# Server and py-cml tests read all variables from this file (no hardcoded fallbacks).
# Set AUTH__API_KEY, EMBEDDING__DIMENSIONS, and DB URLs so tests run without skipping.
# =============================================================================

# [Optional] Use existing Postgres from env instead of testcontainers (server integration tests)
# USE_ENV_DB=1
# DATABASE__POSTGRES_URL=postgresql+asyncpg://...

# py-cml integration/e2e: tests load repo root .env and use CML_BASE_URL (or CML_TEST_URL)
# and CML_API_KEY / AUTH__API_KEY. Use same AUTH__API_KEY as the server (e.g. test-key).
# CML_TEST_URL=http://localhost:8000
# CML_TEST_API_KEY=   (optional; falls back to AUTH__API_KEY)
