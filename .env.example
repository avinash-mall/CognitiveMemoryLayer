# =============================================================================
# Cognitive Memory Layer (CML) — Environment Configuration
# =============================================================================
# Copy to .env and set values. [Required] = must set. [Optional] = has default.
# Tests (server and py-cml) read from .env; no hardcoded fallbacks. Set required
# vars below so tests and examples run without skipping.
# =============================================================================

# ---------- PACKAGE VERSION (build) ----------
# VERSION — [Optional] Package version for Hatch build. Used by: pyproject.toml (dynamic version). Fallback: VERSION env or repo VERSION file. Bump when releasing.
# VERSION=1.3.2

# =============================================================================
# PART 1 — CML SERVER (API) — from src/core/config.py
# =============================================================================
# Server reads settings via pydantic-settings. Nested keys use double
# underscore: SECTION__KEY (e.g. DATABASE__POSTGRES_URL). Values match
# docker-compose for local development where relevant.
# =============================================================================

# ---------- REQUIRED ----------

# AUTH__API_KEY — [Required] API key for memory operations (X-API-Key header).
# Used by: src/core/config.py → AuthSettings.api_key; src/api/auth.py.
# Use test-key for local dev (py-cml tests); set strong secret in production.
AUTH__API_KEY=test-key

# DATABASE__POSTGRES_URL — [Required] PostgreSQL connection string.
# Used by: src/core/config.py → DatabaseSettings.postgres_url; src/storage/connection.py.
# Format: postgresql+asyncpg://user:password@host:port/dbname
DATABASE__POSTGRES_URL=postgresql+asyncpg://memory:memory@localhost/memory

# ---------- OPTIONAL (defaults shown) ----------

# DATABASE__NEO4J_URL — [Optional] Neo4j Bolt URL. Used by: config.py → DatabaseSettings; src/storage/neo4j.py.
# DATABASE__NEO4J_URL=bolt://localhost:7687

# DATABASE__NEO4J_USER — [Optional] Neo4j username. Used by: config.py → DatabaseSettings; src/storage/neo4j.py.
# DATABASE__NEO4J_USER=neo4j

# DATABASE__NEO4J_PASSWORD — [Optional] Neo4j password. Set when Neo4j is secured. Default: empty. Used by: config.py; neo4j.py.
# DATABASE__NEO4J_PASSWORD=

# DATABASE__REDIS_URL — [Optional] Redis connection for cache, Celery broker. Used by: config.py; src/storage/redis.py; celery_app.py.
# DATABASE__REDIS_URL=redis://localhost:6379

# AUTH__ADMIN_API_KEY — [Optional] Admin API key for dashboard, consolidate, run_forgetting, list_tenants. Default: None. Used by: config.py; auth.py.
# AUTH__ADMIN_API_KEY=

# AUTH__DEFAULT_TENANT_ID — [Optional] Default tenant when X-Tenant-ID not sent. Used by: config.py → AuthSettings.
# AUTH__DEFAULT_TENANT_ID=default

# AUTH__RATE_LIMIT_REQUESTS_PER_MINUTE — [Optional] Per-tenant rate limit; 0 = disable. Used by: config.py; app.py; dashboard_routes.py.
# AUTH__RATE_LIMIT_REQUESTS_PER_MINUTE=60

# LLM__PROVIDER — [Optional] LLM provider: openai | openai_compatible | ollama | gemini | claude. Used by: config.py → LLMSettings; src/utils/llm.py.
# LLM__PROVIDER=openai

# LLM__MODEL — [Optional] Model name. Used by: config.py; llm.py (get_llm_client, get_internal_llm_client fallback).
# LLM__MODEL=gpt-4o-mini

# LLM__API_KEY — [Optional] API key; fallback: OPENAI_API_KEY. Used by: config.py; llm.py.
# LLM__API_KEY=

# LLM__BASE_URL — [Optional] OpenAI-compatible endpoint for openai_compatible/ollama. API in Docker, Ollama on host: http://host.docker.internal:11434/v1
# LLM__BASE_URL=

# LLM_INTERNAL__* — [Optional] Optional separate LLM for internal tasks (~1 call/write, 1–2/read, ~5–10/turn). Used by: config.py → LLMInternalSettings; llm.py get_internal_llm_client. If unset, LLM__* is used.
# LLM_INTERNAL__PROVIDER=ollama
# LLM_INTERNAL__MODEL=llama3.2:3b
# LLM_INTERNAL__BASE_URL=http://localhost:11434/v1
# LLM_INTERNAL__API_KEY=

# EMBEDDING__PROVIDER — [Optional] Embedding provider: openai | local | openai_compatible | ollama. Used by: config.py; src/utils/embeddings.py.
# EMBEDDING__PROVIDER=openai

# EMBEDDING__MODEL — [Optional] Embedding model name. Used by: config.py; embeddings.py.
# EMBEDDING__MODEL=text-embedding-3-small

# EMBEDDING__DIMENSIONS — [Optional] Vector dimension; must match model output and DB schema. Used by: config.py; embeddings.py; validate_embedding_dimensions; storage/models.py. If changed: docker compose down -v, re-run migrations.
# EMBEDDING__DIMENSIONS=1536

# EMBEDDING__LOCAL_MODEL — [Optional] Model for provider=local (sentence-transformers). Used by: config.py; embeddings.py.
# EMBEDDING__LOCAL_MODEL=all-MiniLM-L6-v2

# EMBEDDING__API_KEY — [Optional] Fallback: OPENAI_API_KEY. Used by: config.py; embeddings.py.
# EMBEDDING__API_KEY=

# EMBEDDING__BASE_URL — [Optional] OpenAI-compatible embedding endpoint. API in Docker: http://host.docker.internal:11434/v1
# EMBEDDING__BASE_URL=

# CHUNKER__TOKENIZER — [Optional] Hugging Face tokenizer ID for semchunk. Used by: config.py → ChunkerSettings; src/memory/working/manager.py.
# CHUNKER__TOKENIZER=google/flan-t5-base

# CHUNKER__CHUNK_SIZE — [Optional] Max tokens per chunk. Used by: config.py; working/manager.py.
# CHUNKER__CHUNK_SIZE=500

# CHUNKER__OVERLAP_PERCENT — [Optional] Chunk overlap ratio 0–1. Used by: config.py; working/chunker.py.
# CHUNKER__OVERLAP_PERCENT=0.15

# APP_NAME — [Optional] Application name. Used by: config.py → Settings.
# APP_NAME=CognitiveMemoryLayer

# DEBUG — [Optional] Enable debug mode (verbose 500 errors). Used by: config.py; routes.py _safe_500_detail.
# DEBUG=false

# CORS_ORIGINS — [Optional] Comma-separated origins; unset = default list; DEBUG=true allows "*". Used by: config.py; app.py.
# CORS_ORIGINS=

# Feature flags — [Optional] All default true except STORE_ASYNC. Used by: config.py → FeatureFlags; orchestrator, hippocampal/store, classifier, conflict_detector, etc. See UsageDocumentation.md.
# FEATURES__STABLE_KEYS_ENABLED=true             # [Optional] SHA256-based stable keys for consolidation facts
# FEATURES__WRITE_TIME_FACTS_ENABLED=true        # [Optional] Populate semantic store at write time
# FEATURES__BATCH_EMBEDDINGS_ENABLED=true        # [Optional] Single embed_batch() per turn
# FEATURES__STORE_ASYNC=false                    # [Optional] Enqueue turn writes to Redis (reduces latency; requires Redis)
# FEATURES__CACHED_EMBEDDINGS_ENABLED=true       # [Optional] Cache embeddings in Redis (requires Redis)
# FEATURES__RETRIEVAL_TIMEOUTS_ENABLED=true      # [Optional] Per-step asyncio.wait_for timeouts (planner.py)
# FEATURES__SKIP_IF_FOUND_CROSS_GROUP=true       # [Optional] Skip remaining steps on fact hit (retriever.py)
# FEATURES__DB_DEPENDENCY_COUNTS=true            # [Optional] DB-side aggregation for forgetting
# FEATURES__BOUNDED_STATE_ENABLED=true           # [Optional] LRU+TTL state maps (working memory)
# FEATURES__HNSW_EF_SEARCH_TUNING=true           # [Optional] Query-time HNSW ef_search (postgres.py)
# FEATURES__CONSTRAINT_EXTRACTION_ENABLED=true   # [Optional] Extract goals, values, policies at write time
# FEATURES__USE_LLM_CONSTRAINT_EXTRACTOR=true    # [Optional] LLM for constraints (vs rule-based); orchestrator, hippocampal/store
# FEATURES__USE_LLM_WRITE_TIME_FACTS=true        # [Optional] LLM for write-time facts; orchestrator
# FEATURES__USE_LLM_QUERY_CLASSIFIER_ONLY=true   # [Optional] Skip fast pattern path; always use LLM; classifier.py
# FEATURES__USE_LLM_SALIENCE_REFINEMENT=true     # [Optional] Unified extractor salience; hippocampal/store, write_gate
# FEATURES__USE_LLM_PII_REDACTION=true           # [Optional] Unified extractor PII spans; hippocampal/store, write_gate
# FEATURES__USE_LLM_WRITE_GATE_IMPORTANCE=true   # [Optional] Unified extractor importance; hippocampal/store, write_gate
# FEATURES__USE_LLM_CONFLICT_DETECTION_ONLY=true # [Optional] Skip fast path for conflict detection; conflict_detector.py

# Retrieval — [Optional] Used by: config.py → RetrievalSettings; planner.py, retriever.py, packet_builder.py, postgres.py.
# RETRIEVAL__DEFAULT_STEP_TIMEOUT_MS=5000        # [Optional] Per-step timeout (ms)
# RETRIEVAL__TOTAL_TIMEOUT_MS=15000              # [Optional] Total retrieval budget (ms)
# RETRIEVAL__HNSW_EF_SEARCH=40                   # [Optional] pgvector HNSW ef_search base; max(this, top_k) when tuning on

# =============================================================================
# PART 2 — PY-CML CLIENT (packages/py-cml)
# =============================================================================
# Used when connecting to the CML API from Python. Loaded from env or .env;
# constructor args override env.
# =============================================================================

# ---------- REQUIRED ----------

# CML_API_KEY — [Required] API key (sent as X-API-Key). Used by: py-cml config.py → CMLConfig; client requests. Use test-key for local dev.
CML_API_KEY=test-key

# CML_BASE_URL — [Required] CML server base URL. Used by: py-cml config.py; no default in code.
CML_BASE_URL=http://localhost:8000

# ---------- OPTIONAL (defaults shown) ----------

# CML_TENANT_ID — [Optional] Tenant identifier (X-Tenant-ID). Used by: py-cml config.py. Default: default
# CML_TENANT_ID=default

# CML_TIMEOUT — [Optional] Request timeout (seconds). Used by: py-cml config.py; client requests. Default: 30.0
# CML_TIMEOUT=30.0

# CML_MAX_RETRIES — [Optional] Max retry attempts. Used by: py-cml config.py; client requests. Default: 3
# CML_MAX_RETRIES=3

# CML_RETRY_DELAY — [Optional] Base delay between retries (seconds). Used by: py-cml config.py; exponential backoff. Default: 1.0
# CML_RETRY_DELAY=1.0

# CML_MAX_RETRY_DELAY — [Optional] Max backoff delay (seconds). Used by: py-cml config.py; caps exponential backoff. Default: 60.0
# CML_MAX_RETRY_DELAY=60.0

# CML_VERIFY_SSL — [Optional] Verify SSL certificates. Used by: py-cml config.py; client requests. Default: true
# CML_VERIFY_SSL=true

# CML_ADMIN_API_KEY — [Optional] Admin API key for consolidate, run_forgetting, list_tenants. Used by: py-cml config.py.
# CML_ADMIN_API_KEY=

# ---------- EMBEDDED MODE [Optional] ----------
# EmbeddedCognitiveMemoryLayer reads these from .env when not set in code.
# EMBEDDING__MODEL — [Optional] Model for local embeddings. Used by: py-cml embedded config.
# EMBEDDING__DIMENSIONS — [Optional] Vector dimension; default 384; should match server when syncing.
# EMBEDDING__BASE_URL — [Optional] OpenAI-compatible embedding endpoint.
# LLM__MODEL — [Optional] Model for embedded LLM. Used by: py-cml embedded config.
# LLM__BASE_URL — [Optional] LLM endpoint for embedded mode.

# =============================================================================
# PART C — FALLBACK API KEYS
# =============================================================================
# Fallback when LLM__API_KEY or EMBEDDING__API_KEY are unset. Used by: src/utils/llm.py; src/utils/embeddings.py.
# =============================================================================

# OPENAI_API_KEY — [Optional] Fallback for LLM__API_KEY, EMBEDDING__API_KEY. Required for OpenAI/Ollama cloud providers.
# OPENAI_API_KEY=your-openai-key

# =============================================================================
# PART D — EXAMPLES & SCRIPTS
# =============================================================================
# Used by repo examples/ and tests; not part of core server or py-cml config.
# =============================================================================

# MEMORY_API_URL — [Optional] CML API URL for repo examples. Used by: examples; fallback for py-cml CML_BASE_URL (config.py env_map).
# MEMORY_API_URL=http://localhost:8000

# MEMORY_API_KEY — [Optional] Examples use this or AUTH__API_KEY.
# MEMORY_API_KEY=

# MEMORY_API_TIMEOUT — [Optional] Timeout for examples (e.g. ollama_chat_test.py).
# MEMORY_API_TIMEOUT=120

# OLLAMA_BASE_URL — [Optional] Ollama API URL for Ollama examples. Used by: examples.
# OLLAMA_BASE_URL=http://localhost:11434/v1

# ANTHROPIC_API_KEY — [Optional] Used by examples/tool-calling.
# ANTHROPIC_API_KEY=

# =============================================================================
# PART F — DEVELOPMENT & TESTING
# =============================================================================
# Server and py-cml tests read all variables from this file (no hardcoded fallbacks).
# Set AUTH__API_KEY, EMBEDDING__DIMENSIONS, and DB URLs so tests run without skipping.
# =============================================================================

# USE_ENV_DB — [Optional] Use existing Postgres instead of testcontainers. Used by: server integration tests.
# USE_ENV_DB=1

# py-cml integration/e2e — tests load repo root .env; use CML_BASE_URL or CML_TEST_URL, CML_API_KEY or CML_TEST_API_KEY. Use same AUTH__API_KEY as the server (e.g. test-key).
# CML_TEST_URL — [Optional] Override CML_BASE_URL for py-cml tests. Used by: tests/conftest.
# CML_TEST_URL=http://localhost:8000
# CML_TEST_API_KEY — [Optional] Override CML_API_KEY for py-cml tests; falls back to AUTH__API_KEY.
# CML_TEST_API_KEY=
