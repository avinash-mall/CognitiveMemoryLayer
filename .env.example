# Database Configuration (values match docker/docker-compose.yml for local dev)
# Postgres: user/password must match your server; Docker uses memory/memory
DATABASE__POSTGRES_URL=postgresql+asyncpg://memory:memory@localhost/memory
DATABASE__NEO4J_URL=bolt://localhost:7687
DATABASE__NEO4J_USER=neo4j
# Matches Docker NEO4J_AUTH (neo4j/password)
DATABASE__NEO4J_PASSWORD=password
DATABASE__REDIS_URL=redis://localhost:6379

# API Configuration
AUTH__API_KEY=your-api-key-here
AUTH__ADMIN_API_KEY=your-admin-key-here
AUTH__DEFAULT_TENANT_ID=default

# Optional fallback for LLM and Embedding API keys (used if LLM__API_KEY / EMBEDDING__API_KEY unset)
# OPENAI_API_KEY=your-openai-key

# LLM Configuration (provider: openai | vllm | ollama | gemini | claude)
LLM__PROVIDER=openai
LLM__MODEL=gpt-4o-mini
LLM__API_KEY=your-openai-key
# LLM__BASE_URL= optional; set for vllm/ollama or OpenAI-compatible proxy

# Example: vLLM local server
# LLM__PROVIDER=vllm
# LLM__MODEL=meta-llama/Llama-3.2-1B-Instruct
# LLM__BASE_URL=http://localhost:8000/v1

# Example: Ollama local
# LLM__PROVIDER=ollama
# LLM__MODEL=llama3.2
# LLM__BASE_URL=http://localhost:11434/v1

# Embedding Configuration (provider: openai | local | vllm)
EMBEDDING__PROVIDER=openai
EMBEDDING__MODEL=text-embedding-3-small
EMBEDDING__DIMENSIONS=1536
EMBEDDING__API_KEY=your-openai-key
# EMBEDDING__LOCAL_MODEL=all-MiniLM-L6-v2
# EMBEDDING__BASE_URL= optional; required for vllm; OpenAI-compatible embedding endpoint

# Example: local vLLM embeddings (OpenAI-compatible endpoint)
# EMBEDDING__PROVIDER=vllm
# EMBEDDING__MODEL=your-embedding-model
# EMBEDDING__DIMENSIONS=384
# EMBEDDING__BASE_URL=http://localhost:8000/v1

# Optional: CORS (comma-separated origins; leave unset for defaults)
# CORS_ORIGINS=http://localhost:3000,http://localhost:8080

# Optional: Debug mode (allow all CORS origins when true)
# DEBUG=false
